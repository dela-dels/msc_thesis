{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342bd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# FX Buying Price Forecast Toolkit\n",
    "# SARIMAX + XGBoost + TS-CV + SHAP\n",
    "# ================================\n",
    "\n",
    "# --- 0) Imports & Toggles ---\n",
    "import pathlib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001a0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional heavy steps\n",
    "RUN_SARIMAX = True        # set False if statsmodels too slow\n",
    "RUN_XGBOOST = True        # set False to skip XGB (will fallback to RF)\n",
    "RUN_SHAP = True           # set False to skip SHAP\n",
    "N_SPLITS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f597e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try XGBoost; fallback to RandomForest\n",
    "XGB_AVAILABLE, SHAP_AVAILABLE = False, False\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# SARIMAX\n",
    "if RUN_SARIMAX:\n",
    "    try:\n",
    "        from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    except Exception:\n",
    "        RUN_SARIMAX = False\n",
    "        print(\"SARIMAX import failed; skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00926a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Load Data ---\n",
    "DATA_PATH = \"datasets/cleaned/merged_fx_dataset.csv\"  # change if needed\n",
    "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\"\n",
    "\n",
    "raw = pd.read_csv(DATA_PATH)\n",
    "raw.columns = [str(c).strip() for c in raw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b0c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date handling\n",
    "date_col = [c for c in raw.columns if c.lower() == \"date\"]\n",
    "assert len(date_col) == 1, \"Expected a single 'Date' column.\"\n",
    "DATE_COL = date_col[0]\n",
    "\n",
    "raw[DATE_COL] = pd.to_datetime(raw[DATE_COL], errors=\"coerce\")\n",
    "raw = raw.dropna(subset=[DATE_COL]).sort_values(DATE_COL).set_index(DATE_COL)\n",
    "\n",
    "# Numeric coercion\n",
    "for c in raw.columns:\n",
    "    raw[c] = pd.to_numeric(raw[c], errors=\"coerce\")\n",
    "\n",
    "# Fill slow-moving macro (monthly/quarterly)\n",
    "df = raw.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cce5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(data: pd.DataFrame):\n",
    "    d = data.copy()\n",
    "    assert \"Buying\" in d.columns, \"Target column 'Buying' not found.\"\n",
    "\n",
    "    # Lags & rolling stats of target\n",
    "    lag_list = [1, 5, 10, 21, 63]\n",
    "    for L in lag_list:\n",
    "        d[f\"Buy_lag{L}\"] = d[\"Buying\"].shift(L)\n",
    "\n",
    "    for W in [5, 21, 63]:\n",
    "        d[f\"Buy_roll_mean_{W}\"] = d[\"Buying\"].shift(1).rolling(W).mean()\n",
    "        d[f\"Buy_roll_std_{W}\"] = d[\"Buying\"].shift(1).rolling(W).std()\n",
    "\n",
    "    # Spreads (GH - US)\n",
    "    if \"GhInflationRate\" in d.columns and \"USInflationRate\" in d.columns:\n",
    "        d[\"Inflation_Spread\"] = d[\"GhInflationRate\"] - d[\"USInflationRate\"]\n",
    "    if \"GhInterestRate\" in d.columns and \"USInterestRate\" in d.columns:\n",
    "        d[\"Rate_Spread\"] = d[\"GhInterestRate\"] - d[\"USInterestRate\"]\n",
    "\n",
    "    # Commodity log returns\n",
    "    for col in [\"BrentOil\", \"Gold\", \"Cocoa\"]:\n",
    "        if col in d.columns:\n",
    "            d[f\"{col}_ret\"] = np.log(d[col].replace(0, np.nan)).diff()\n",
    "\n",
    "    # Macro deltas (fast changes)\n",
    "    for col in [\"NIR\", \"NetForeignAssets\", \"Imports\", \"Exports\"]:\n",
    "        if col in d.columns:\n",
    "            d[f\"{col}_chg\"] = d[col].diff()\n",
    "\n",
    "    # Calendar\n",
    "    d[\"Month\"] = d.index.month\n",
    "    d[\"Quarter\"] = d.index.quarter\n",
    "    d[\"Dow\"] = d.index.dayofweek\n",
    "\n",
    "    # Level exogenous that might help (will be lagged)\n",
    "    base_levels = [c for c in [\n",
    "        \"MPR\", \"GhInflationRate\", \"USInflationRate\", \"GhInterestRate\", \"USInterestRate\",\n",
    "        \"NIR\", \"NetForeignAssets\", \"Imports\", \"Exports\"\n",
    "    ] if c in d.columns]\n",
    "\n",
    "    # XGBoost feature set (wide)\n",
    "    feat_cols = []\n",
    "    feat_cols += [f\"Buy_lag{L}\" for L in lag_list]\n",
    "    feat_cols += [f\"Buy_roll_mean_{W}\" for W in [5, 21, 63]]\n",
    "    feat_cols += [f\"Buy_roll_std_{W}\" for W in [5, 21, 63]]\n",
    "    feat_cols += [c for c in [\"Inflation_Spread\",\n",
    "                              \"Rate_Spread\"] if c in d.columns]\n",
    "    feat_cols += [f\"{c}_ret\" for c in [\"BrentOil\",\n",
    "                                       \"Gold\", \"Cocoa\"] if f\"{c}_ret\" in d.columns]\n",
    "    feat_cols += [f\"{c}_chg\" for c in [\"NIR\", \"NetForeignAssets\",\n",
    "                                       \"Imports\", \"Exports\"] if f\"{c}_chg\" in d.columns]\n",
    "    feat_cols += base_levels + [\"Month\", \"Quarter\", \"Dow\"]\n",
    "\n",
    "    # Shift ALL features by 1 step: features(t-1) -> predict Buying(t)\n",
    "    d[feat_cols] = d[feat_cols].shift(1)\n",
    "\n",
    "    # Target\n",
    "    d[\"y\"] = d[\"Buying\"]\n",
    "\n",
    "    d = d.dropna()\n",
    "\n",
    "    # Reduced exog for SARIMAX (stable set)\n",
    "    sarimax_exog = [c for c in [\n",
    "        \"Inflation_Spread\", \"Rate_Spread\", \"BrentOil_ret\", \"Gold_ret\", \"Cocoa_ret\", \"MPR\", \"NIR\"\n",
    "    ] if c in d.columns]\n",
    "\n",
    "    return d, feat_cols, sarimax_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "338c61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "work, XFEATS, SARIMAX_EXOG = make_features(df)\n",
    "X = work[XFEATS].copy()\n",
    "y = work[\"y\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9f308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Helpers: Metrics & CV ---\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "684ea1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "\n",
    "records = []\n",
    "pred_logs = []   # to plot last fold\n",
    "models_trained = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed51c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_XGBOOST:\n",
    "    if XGB_AVAILABLE:\n",
    "        xgb = XGBRegressor(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=5,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=42, objective=\"reg:squarederror\"\n",
    "        )\n",
    "        xgb_name = \"XGBoost\"\n",
    "    else:\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        xgb = RandomForestRegressor(\n",
    "            n_estimators=500, random_state=42, n_jobs=-1)\n",
    "        xgb_name = \"RandomForest(fallback)\"\n",
    "\n",
    "    fold = 0\n",
    "    for tr, va in tscv.split(X):\n",
    "        fold += 1\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        xgb.fit(Xtr, ytr)\n",
    "        yhat = xgb.predict(Xva)\n",
    "\n",
    "        mae, rmse, r2 = eval_metrics(yva, yhat)\n",
    "        records.append({\"model\": xgb_name, \"fold\": fold,\n",
    "                       \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "        pred_logs.append({\"model\": xgb_name, \"fold\": fold,\n",
    "                         \"dates\": Xva.index, \"y_true\": yva, \"y_pred\": yhat})\n",
    "\n",
    "    models_trained[xgb_name] = (xgb, XFEATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "038529a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SARIMAX and len(SARIMAX_EXOG) > 0:\n",
    "    fold = 0\n",
    "    for tr, va in tscv.split(work):\n",
    "        fold += 1\n",
    "        ytr, yva = y.iloc[tr], y.iloc[va]\n",
    "        Xtr_ex, Xva_ex = work[SARIMAX_EXOG].iloc[tr], work[SARIMAX_EXOG].iloc[va]\n",
    "\n",
    "        try:\n",
    "            sarimax = SARIMAX(\n",
    "                endog=ytr, exog=Xtr_ex,\n",
    "                order=(1, 1, 1),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False\n",
    "            )\n",
    "            res = sarimax.fit(disp=False)\n",
    "            yhat = res.forecast(steps=len(va), exog=Xva_ex)\n",
    "\n",
    "            mae, rmse, r2 = eval_metrics(yva, yhat)\n",
    "            records.append({\"model\": \"SARIMAX(1,1,1)+exog\",\n",
    "                           \"fold\": fold, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "            pred_logs.append({\"model\": \"SARIMAX(1,1,1)+exog\", \"fold\": fold,\n",
    "                             \"dates\": yva.index, \"y_true\": yva, \"y_pred\": yhat.values})\n",
    "        except Exception as e:\n",
    "            records.append({\"model\": \"SARIMAX(1,1,1)+exog\", \"fold\": fold,\n",
    "                           \"MAE\": np.nan, \"RMSE\": np.nan, \"R2\": np.nan, \"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32b1cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cross-validated Performance (averaged) ===\n",
      "                          MAE      RMSE        R2\n",
      "model                                            \n",
      "SARIMAX(1,1,1)+exog  0.736586  1.058551 -2.573688\n",
      "XGBoost              0.811151  1.109448 -2.935091\n"
     ]
    }
   ],
   "source": [
    "# --- 6) Metrics summary ---\n",
    "metrics_df = pd.DataFrame(records)\n",
    "summary = (\n",
    "    metrics_df.groupby(\"model\")[[\"MAE\", \"RMSE\", \"R2\"]\n",
    "                                ].mean().sort_values(\"RMSE\")\n",
    "    if not metrics_df.empty else pd.DataFrame(columns=[\"MAE\", \"RMSE\", \"R2\"])\n",
    ")\n",
    "print(\"\\n=== Cross-validated Performance (averaged) ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e98e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = pathlib.Path(\"./fx_outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c18475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved prediction plots:\n",
      " - fx_outputs/XGBoost_last_fold.png\n",
      " - fx_outputs/SARIMAX1,1,1_exog_last_fold.png\n"
     ]
    }
   ],
   "source": [
    "last_fold = {}\n",
    "for rec in pred_logs:\n",
    "    last_fold[rec[\"model\"]] = rec  # keep last occurrence (latest fold)\n",
    "\n",
    "plot_paths = []\n",
    "for model_name, rec in last_fold.items():\n",
    "    dates = pd.to_datetime(rec[\"dates\"])\n",
    "    yt = np.asarray(rec[\"y_true\"], float)\n",
    "    yp = np.asarray(rec[\"y_pred\"], float)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(dates, yt, label=\"Actual\")\n",
    "    plt.plot(dates, yp, label=\"Predicted\")\n",
    "    plt.title(f\"{model_name} — Last Fold Predictions\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Buying\")\n",
    "    plt.legend()\n",
    "    fp = out_dir / \\\n",
    "        f\"{model_name.replace(' ', '_').replace('(', '').replace(')', '').replace('+', '_')}_last_fold.png\"\n",
    "    plt.savefig(fp, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    plot_paths.append(str(fp))\n",
    "\n",
    "print(\"\\nSaved prediction plots:\")\n",
    "for p in plot_paths:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b812d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature importance saved: fx_outputs/feature_importance_top20.csv and fx_outputs/feature_importance_top20.png\n",
      "SHAP bar summary saved: fx_outputs/shap_summary_bar.png\n"
     ]
    }
   ],
   "source": [
    "if RUN_XGBOOST:\n",
    "    model_name = list(models_trained.keys())[0]\n",
    "    model, feat_cols = models_trained[model_name]\n",
    "\n",
    "    # Built-in feature importance (Top 20)\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        fi = pd.DataFrame(\n",
    "            {\"feature\": feat_cols, \"importance\": model.feature_importances_})\n",
    "        fi = fi.sort_values(\"importance\", ascending=False)\n",
    "        fi_path = out_dir / \"feature_importance_top20.csv\"\n",
    "        fi.head(20).to_csv(fi_path, index=False)\n",
    "\n",
    "        top = fi.head(20).iloc[::-1]\n",
    "        plt.figure()\n",
    "        plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "        plt.title(f\"{model_name} Feature Importance (Top 20)\")\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        fp = out_dir / \"feature_importance_top20.png\"\n",
    "        plt.savefig(fp, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(\"\\nFeature importance saved:\", fi_path, \"and\", fp)\n",
    "\n",
    "    # SHAP (optional)\n",
    "    if RUN_SHAP and SHAP_AVAILABLE and XGB_AVAILABLE and model_name.startswith(\"XGBoost\"):\n",
    "        sample_n = min(2000, X.shape[0])\n",
    "        X_sample = X.sample(sample_n, random_state=42)\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer(X_sample)\n",
    "\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "        fp = out_dir / \"shap_summary_bar.png\"\n",
    "        plt.savefig(fp, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(\"SHAP bar summary saved:\", fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1152d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Files saved ===\n",
      " - Metrics (all folds): fx_outputs/fx_model_metrics_all_folds.csv\n",
      " - Predictions (all folds): fx_outputs/fx_model_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 9) Save full metrics & per-timestamp predictions ---\n",
    "metrics_csv = out_dir / \"fx_model_metrics_all_folds.csv\"\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "\n",
    "pred_rows = []\n",
    "for rec in pred_logs:\n",
    "    for dt, yt, yp in zip(rec[\"dates\"], rec[\"y_true\"], rec[\"y_pred\"]):\n",
    "        pred_rows.append({\n",
    "            \"model\": rec[\"model\"],\n",
    "            \"fold\": rec[\"fold\"],\n",
    "            \"date\": pd.to_datetime(dt),\n",
    "            \"y_true\": float(yt),\n",
    "            \"y_pred\": float(yp)\n",
    "        })\n",
    "pred_df = pd.DataFrame(pred_rows).sort_values([\"model\", \"fold\", \"date\"])\n",
    "pred_csv = out_dir / \"fx_model_predictions.csv\"\n",
    "pred_df.to_csv(pred_csv, index=False)\n",
    "\n",
    "print(\"\\n=== Files saved ===\")\n",
    "print(\" - Metrics (all folds):\", metrics_csv)\n",
    "print(\" - Predictions (all folds):\", pred_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe978668",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 181\u001b[39m\n\u001b[32m    178\u001b[39m Xtr, Xva = X.iloc[tr], X.iloc[va]\n\u001b[32m    179\u001b[39m ytr, yva = y.iloc[tr], y.iloc[va]\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m yhat = xgb.predict(Xva)\n\u001b[32m    184\u001b[39m mae, rmse, r2 = eval_metrics(yva, yhat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/thesis/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/thesis/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/thesis/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/thesis/.venv/lib/python3.12/site-packages/xgboost/training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/thesis/.venv/lib/python3.12/site-packages/xgboost/core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 4) XGBoost (or RF fallback) ---\n",
    "\n",
    "\n",
    "\n",
    "# --- 5) SARIMAX ---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 7) Plots (last fold of each model) ---\n",
    "\n",
    "\n",
    "\n",
    "# --- 8) Feature importance & SHAP (tree models) ---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbd0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fff787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                MAE     RMSE        R2\n",
      "model                                                 \n",
      "GradientBoostingRegressor  1.925321  2.50554 -1.540633\n",
      "One-step ahead forecast (level): 14.6898\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GradientBoostingRegressor for USD/GHS \"Buying\" (time-series)\n",
    "# - Leak-safe lags/rolls\n",
    "# - TimeSeriesSplit CV (MAE/RMSE/R2)\n",
    "# - Last-fold Actual vs Predicted plot\n",
    "# - Feature importance (CSV + bar chart)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "DATA_PATH = \"datasets/cleaned/merged_fx_dataset.csv\"   # <-- change if needed\n",
    "OUT_DIR = \"./fx_gbr_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- 1) Load ----------\n",
    "raw = pd.read_csv(DATA_PATH)\n",
    "raw.columns = [str(c).strip() for c in raw.columns]\n",
    "date_col = [c for c in raw.columns if c.lower() == \"date\"]\n",
    "assert len(date_col) == 1, \"Expected a single 'Date' column.\"\n",
    "DATE = date_col[0]\n",
    "\n",
    "raw[DATE] = pd.to_datetime(raw[DATE], errors=\"coerce\")\n",
    "raw = raw.dropna(subset=[DATE]).sort_values(DATE).set_index(DATE)\n",
    "\n",
    "# numeric coercion + sensible fill for slow-moving series\n",
    "for c in raw.columns:\n",
    "    raw[c] = pd.to_numeric(raw[c], errors=\"coerce\")\n",
    "df = raw.ffill().bfill()\n",
    "\n",
    "# ---------- 2) Features (NO LEAKAGE) ----------\n",
    "\n",
    "\n",
    "def make_features(data: pd.DataFrame):\n",
    "    d = data.copy()\n",
    "    if \"Buying\" not in d.columns:\n",
    "        raise ValueError(\"Target 'Buying' not found.\")\n",
    "\n",
    "    # target lags\n",
    "    for L in [1, 5, 10, 21]:\n",
    "        d[f\"Buy_lag{L}\"] = d[\"Buying\"].shift(L)\n",
    "\n",
    "    # rolling stats (shifted to avoid leakage)\n",
    "    for W in [5, 21]:\n",
    "        d[f\"Buy_roll_mean_{W}\"] = d[\"Buying\"].shift(1).rolling(W).mean()\n",
    "        d[f\"Buy_roll_std_{W}\"] = d[\"Buying\"].shift(1).rolling(W).std()\n",
    "\n",
    "    # spreads (GH - US)\n",
    "    if \"GhInflationRate\" in d.columns and \"USInflationRate\" in d.columns:\n",
    "        d[\"Inflation_Spread\"] = d[\"GhInflationRate\"] - d[\"USInflationRate\"]\n",
    "    if \"GhInterestRate\" in d.columns and \"USInterestRate\" in d.columns:\n",
    "        d[\"Rate_Spread\"] = d[\"GhInterestRate\"] - d[\"USInterestRate\"]\n",
    "\n",
    "    # commodity log returns\n",
    "    for col in [\"BrentOil\", \"Gold\", \"Cocoa\"]:\n",
    "        if col in d.columns:\n",
    "            d[f\"{col}_ret\"] = np.log(d[col].replace(0, np.nan)).diff()\n",
    "\n",
    "    # macro deltas\n",
    "    for col in [\"NIR\", \"NetForeignAssets\", \"Imports\", \"Exports\"]:\n",
    "        if col in d.columns:\n",
    "            d[f\"{col}_chg\"] = d[col].diff()\n",
    "\n",
    "    # calendar\n",
    "    d[\"Month\"] = d.index.month\n",
    "    d[\"Quarter\"] = d.index.quarter\n",
    "    d[\"Dow\"] = d.index.dayofweek\n",
    "\n",
    "    feat_cols = []\n",
    "    feat_cols += [f\"Buy_lag{L}\" for L in [1, 5, 10, 21]]\n",
    "    feat_cols += [f\"Buy_roll_mean_{W}\" for W in [5, 21]]\n",
    "    feat_cols += [f\"Buy_roll_std_{W}\" for W in [5, 21]]\n",
    "    feat_cols += [c for c in [\"Inflation_Spread\",\n",
    "                              \"Rate_Spread\"] if c in d.columns]\n",
    "    feat_cols += [f\"{c}_ret\" for c in [\"BrentOil\",\n",
    "                                       \"Gold\", \"Cocoa\"] if f\"{c}_ret\" in d.columns]\n",
    "    feat_cols += [f\"{c}_chg\" for c in [\"NIR\", \"NetForeignAssets\",\n",
    "                                       \"Imports\", \"Exports\"] if f\"{c}_chg\" in d.columns]\n",
    "    feat_cols += [\"Month\", \"Quarter\", \"Dow\"]\n",
    "\n",
    "    # shift ALL features by 1 so we predict y_t with info available at t-1\n",
    "    d[feat_cols] = d[feat_cols].shift(1)\n",
    "\n",
    "    d[\"y\"] = d[\"Buying\"]\n",
    "    d = d.dropna()\n",
    "    return d, feat_cols\n",
    "\n",
    "\n",
    "work, FEATURES = make_features(df)\n",
    "X, y = work[FEATURES], work[\"y\"]\n",
    "\n",
    "# ---------- 3) Model & CV ----------\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.9,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)  # expand to 4–5 if you want\n",
    "rows, preds = [], []\n",
    "fold = 0\n",
    "for tr, va in tscv.split(X):\n",
    "    fold += 1\n",
    "    Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "    ytr, yva = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "    gbr.fit(Xtr, ytr)\n",
    "    yhat = gbr.predict(Xva)\n",
    "\n",
    "    mae = mean_absolute_error(yva, yhat)\n",
    "    rmse = root_mean_squared_error(yva, yhat)\n",
    "    r2 = r2_score(yva, yhat)\n",
    "\n",
    "    rows.append({\"model\": \"GradientBoostingRegressor\",\n",
    "                \"fold\": fold, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "    preds.append({\"fold\": fold, \"dates\": Xva.index, \"y_true\": yva,\n",
    "                 \"y_pred\": pd.Series(yhat, index=Xva.index)})\n",
    "\n",
    "metrics = pd.DataFrame(rows)\n",
    "metrics.to_csv(os.path.join(OUT_DIR, \"gbr_metrics_all_folds.csv\"), index=False)\n",
    "print(metrics.groupby(\"model\")[[\"MAE\", \"RMSE\", \"R2\"]].mean())\n",
    "\n",
    "# ---------- 4) Plot last-fold ----------\n",
    "last = preds[-1]\n",
    "plt.figure()\n",
    "plt.plot(last[\"dates\"], last[\"y_true\"].values, label=\"Actual\")\n",
    "plt.plot(last[\"dates\"], last[\"y_pred\"].values, label=\"Predicted\")\n",
    "plt.title(\"GradientBoostingRegressor — Last Fold Predictions\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Buying\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"gbr_last_fold_predictions.png\"),\n",
    "            bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ---------- 5) Feature importance ----------\n",
    "fi = pd.DataFrame({\"feature\": FEATURES, \"importance\": gbr.feature_importances_}\n",
    "                  ).sort_values(\"importance\", ascending=False)\n",
    "fi.to_csv(os.path.join(OUT_DIR, \"gbr_feature_importance.csv\"), index=False)\n",
    "\n",
    "top = fi.head(20).iloc[::-1]\n",
    "plt.figure()\n",
    "plt.barh(top[\"feature\"], top[\"importance\"])\n",
    "plt.title(\"GBR Feature Importance (Top 20)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.savefig(os.path.join(\n",
    "    OUT_DIR, \"gbr_feature_importance_top20.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# ---------- 6) (Optional) Fit on all data & predict next step ----------\n",
    "# Train on all history:\n",
    "gbr.fit(X, y)\n",
    "\n",
    "# Prepare one-step-ahead features from the most recent row (already lagged/shifted)\n",
    "X_last = X.iloc[[-1]]\n",
    "next_pred = gbr.predict(X_last)[0]\n",
    "print(f\"One-step ahead forecast (level): {next_pred:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
