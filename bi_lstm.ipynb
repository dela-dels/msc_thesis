{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb41ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 00m 29s]\n",
      "val_loss: 0.004003169480711222\n",
      "\n",
      "Best val_loss So Far: 0.0023282570764422417\n",
      "Total elapsed time: 00h 04m 53s\n",
      "\n",
      "--- Fitting Fold 2 ---\n",
      "\n",
      "--- Fitting Fold 3 ---\n",
      "\n",
      "Optimal Hyperparameters Found: {'rnn_layer_type': 'lstm', 'num_rnn_layers': 2, 'units_0': 128, 'dropout_0': 0.2, 'learning_rate': 0.005, 'units_1': 128, 'dropout_1': 0.4}\n",
      "\n",
      "Training final model on full fixed training set...\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step\n",
      "\n",
      "======================================================================\n",
      "--- Final Model Evaluation (Advanced BiLSTM/GRU) on Test Set ---\n",
      "Target: Daily Log Returns (Stationary)\n",
      "TimeSteps (Lookback Window): 60 days\n",
      "----------------------------------------------------------------------\n",
      "Mean Absolute Error (MAE): 0.003373\n",
      "Root Mean Squared Error (RMSE): 0.010068\n",
      "R-squared (R²): -0.1967\n",
      "R-squared Target Achievement: NOT ACHIEVED\n",
      "Optimal RNN Type: lstm\n",
      "Optimal Learning Rate: 0.005\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Deep Learning specific imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "# --- Technical Indicator Functions ---\n",
    "\n",
    "\n",
    "def calculate_macd(series, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculates MACD, Signal Line, and MACD Histogram.\"\"\"\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd = exp1 - exp2\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    return macd, signal_line\n",
    "\n",
    "\n",
    "def calculate_rsi(series, window=14):\n",
    "    \"\"\"Calculates the Relative Strength Index (RSI).\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).ewm(span=window, adjust=False).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).ewm(span=window, adjust=False).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def calculate_atr(df, high_col, low_col, close_col, window=14):\n",
    "    \"\"\"Calculates the Average True Range (ATR).\"\"\"\n",
    "    df['high_low'] = df[high_col] - df[low_col]\n",
    "    df['high_close'] = np.abs(df[high_col] - df[close_col].shift(1))\n",
    "    df['low_close'] = np.abs(df[low_col] - df[close_col].shift(1))\n",
    "    df['TrueRange'] = df[['high_low', 'high_close', 'low_close']].max(axis=1)\n",
    "    atr = df['TrueRange'].ewm(span=window, adjust=False).mean()\n",
    "    df.drop(columns=['high_low', 'high_close',\n",
    "            'low_close', 'TrueRange'], inplace=True)\n",
    "    return atr\n",
    "\n",
    "\n",
    "# --- Global Configurations ---\n",
    "TARGET = 'Buying'\n",
    "TIMESTEPS = 60\n",
    "SPLIT_DATE = '2021-01-01'\n",
    "\n",
    "\n",
    "# --- 1. Data Loading and Feature Augmentation ---\n",
    "\n",
    "df = pd.read_csv(\"datasets/cleaned/merged_fx_dataset.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# To calculate ATR, we need previous day's data. We assume 'Buying' is the Close price,\n",
    "# and use 'Selling' and 'Buying' as proxies for High/Low, since they define the daily range.\n",
    "df['High_Proxy'] = df[['Buying', 'Selling']].max(axis=1)\n",
    "df['Low_Proxy'] = df[['Buying', 'Selling']].min(axis=1)\n",
    "\n",
    "# New Target: Daily Log Return (for stationarity)\n",
    "df['Log_Return'] = np.log(df[TARGET] / df[TARGET].shift(1))\n",
    "NEW_TARGET = 'Log_Return'\n",
    "\n",
    "# --- Feature Augmentation ---\n",
    "# A. Technical Indicators\n",
    "df['RSI_14'] = calculate_rsi(df[TARGET], window=14)\n",
    "df['MACD_12_26'], df['MACD_Signal'] = calculate_macd(df[TARGET])\n",
    "\n",
    "# B. Volatility & Risk Metrics\n",
    "df['ATR_14'] = calculate_atr(\n",
    "    df.copy(), 'High_Proxy', 'Low_Proxy', TARGET, window=14)\n",
    "df['Log_Return_Std_20d'] = df[NEW_TARGET].rolling(window=20).std()\n",
    "\n",
    "# C. Economic Differentials & Base Features (using MidRate as a base for technicals)\n",
    "df.set_index('Date', inplace=True)\n",
    "DROP_COLS = [TARGET, 'Selling', 'MidRate',\n",
    "             'High_Proxy', 'Low_Proxy', NEW_TARGET]\n",
    "FEATURES = df.drop(columns=DROP_COLS).columns.tolist()\n",
    "\n",
    "# Add core features (using same logic as before, just listing here for completeness)\n",
    "df['InterestRate_Diff'] = df['GhInterestRate'] - df['USInterestRate']\n",
    "df['Inflation_Diff'] = df['GhInflationRate'] - df['USInflationRate']\n",
    "df['Trade_Balance'] = df['Exports'] - df['Imports']\n",
    "df['DayOfWeek'] = df.index.dayofweek\n",
    "df['DayOfYear'] = df.index.dayofyear\n",
    "df['Is_Month_End'] = df.index.is_month_end.astype(int)\n",
    "QUARTERLY_SHIFT = 65\n",
    "df['GhGDP_QoQ_Growth'] = (\n",
    "    df['GhGDP'] / df['GhGDP'].shift(QUARTERLY_SHIFT) - 1) * 100\n",
    "df['USGDP_QoQ_Growth'] = (\n",
    "    df['USGDP'] / df['USGDP'].shift(QUARTERLY_SHIFT) - 1) * 100\n",
    "\n",
    "# Finalize feature list\n",
    "NEW_FEATURES = ['RSI_14', 'MACD_12_26', 'MACD_Signal', 'ATR_14', 'Log_Return_Std_20d',\n",
    "                'InterestRate_Diff', 'Inflation_Diff', 'Trade_Balance',\n",
    "                'DayOfWeek', 'DayOfYear', 'Is_Month_End',\n",
    "                'GhGDP_QoQ_Growth', 'USGDP_QoQ_Growth']\n",
    "FINAL_FEATURES = [f for f in FEATURES if f not in DROP_COLS] + NEW_FEATURES\n",
    "\n",
    "# Final clean and split preparation\n",
    "df_clean = df.dropna(subset=[NEW_TARGET] + FINAL_FEATURES)\n",
    "X_data = df_clean[FINAL_FEATURES].values\n",
    "y_data = df_clean[NEW_TARGET].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# --- 2. Scaling, Sequence Creation, and Split (Fixed Logic) ---\n",
    "\n",
    "# Use asof to find the nearest previous index if SPLIT_DATE doesn't exist exactly\n",
    "if SPLIT_DATE in df_clean.index:\n",
    "    split_idx = df_clean.index.get_loc(SPLIT_DATE)\n",
    "else:\n",
    "    split_date_actual = df_clean.index[df_clean.index <= SPLIT_DATE].max()\n",
    "    split_idx = df_clean.index.get_loc(split_date_actual)\n",
    "\n",
    "# Split raw data first\n",
    "X_train_raw, X_test_raw = X_data[:split_idx], X_data[split_idx:]\n",
    "y_train_raw, y_test_raw = y_data[:split_idx], y_data[split_idx:]\n",
    "\n",
    "# Fit scalers ONLY on the training data\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler_X.transform(X_test_raw)\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_raw)\n",
    "y_test_scaled = scaler_y.transform(y_test_raw)\n",
    "\n",
    "# Create sequences\n",
    "\n",
    "\n",
    "def create_sequences(X, y, timesteps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - timesteps):\n",
    "        X_seq.append(X[i:i + timesteps, :])\n",
    "        y_seq.append(y[i + timesteps, 0])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(\n",
    "    X_train_scaled, y_train_scaled, TIMESTEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(\n",
    "    X_test_scaled, y_test_scaled, TIMESTEPS)\n",
    "N_FEATURES = X_train_seq.shape[2]\n",
    "\n",
    "print(f\"Data Prepared. X_train_seq shape: {X_train_seq.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Keras Tuner Model Definition (BiLSTM/GRU Architecture) ---\n",
    "\n",
    "def build_model_bilstm(hp):\n",
    "    \"\"\"Defines the BiLSTM/GRU architecture with searchable hyperparameters.\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Decide between LSTM and GRU\n",
    "    layer_type = hp.Choice('rnn_layer_type', values=['lstm', 'gru'])\n",
    "\n",
    "    # Define the number of RNN layers (1 or 2)\n",
    "    num_rnn_layers = hp.Int('num_rnn_layers', min_value=1, max_value=2, step=1)\n",
    "\n",
    "    for i in range(num_rnn_layers):\n",
    "        rnn_units = hp.Int(f'units_{i}', min_value=32, max_value=128, step=32)\n",
    "\n",
    "        # Select the layer based on the choice\n",
    "        if layer_type == 'lstm':\n",
    "            rnn_layer = LSTM(rnn_units, return_sequences=(\n",
    "                i < num_rnn_layers - 1))\n",
    "        else:  # gru\n",
    "            rnn_layer = GRU(rnn_units, return_sequences=(\n",
    "                i < num_rnn_layers - 1))\n",
    "\n",
    "        # Add Bidirectional wrapper for enhanced sequence context\n",
    "        model.add(Bidirectional(rnn_layer,\n",
    "                                input_shape=(TIMESTEPS, N_FEATURES) if i == 0 else None))\n",
    "\n",
    "        # Searchable dropout rate\n",
    "        model.add(\n",
    "            Dropout(hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # Compile the model with a fine-grained searchable learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[\n",
    "                                 1e-2, 5e-3, 1e-3, 5e-4, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- 4. Hyperparameter Search ---\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model_bilstm,\n",
    "    objective='val_loss',\n",
    "    max_trials=15,  # Increased trials for the larger search space\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner_fx_advanced',\n",
    "    project_name='lstm_fx_advanced',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Keras Tuner Random Search (Advanced Features & BiLSTM)...\")\n",
    "\n",
    "# Perform Walk-Forward Search\n",
    "for fold, (train_index, val_index) in enumerate(tscv.split(X_train_seq)):\n",
    "    print(f\"\\n--- Fitting Fold {fold+1} ---\")\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_seq[train_index], X_train_seq[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_seq[train_index], y_train_seq[val_index]\n",
    "\n",
    "    tuner.search(\n",
    "        X_train_fold, y_train_fold,\n",
    "        epochs=15,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=5)]\n",
    "    )\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\\nOptimal Hyperparameters Found: {best_hps.values}\")\n",
    "\n",
    "# --- 5. Final Model Training and Evaluation ---\n",
    "final_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "print(\"\\nTraining final model on full fixed training set...\")\n",
    "final_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_seq, y_test_seq),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict on the hold-out test set\n",
    "y_pred_scaled = final_model.predict(X_test_seq)\n",
    "\n",
    "# Inverse transform to original Log Return scale\n",
    "y_test_unscaled = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1))\n",
    "y_pred_unscaled = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Calculate final metrics on the Log Returns\n",
    "mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
    "mse = mean_squared_error(y_test_unscaled, y_pred_unscaled)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- Final Model Evaluation (Advanced BiLSTM/GRU) on Test Set ---\")\n",
    "print(\"Target: Daily Log Returns (Stationary)\")\n",
    "print(f\"TimeSteps (Lookback Window): {TIMESTEPS} days\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")\n",
    "print(\n",
    "    f\"R-squared Target Achievement: {'ACHIEVED' if r2 >= 0.70 else 'NOT ACHIEVED'}\")\n",
    "print(f\"Optimal RNN Type: {best_hps.get('rnn_layer_type')}\")\n",
    "print(f\"Optimal Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
